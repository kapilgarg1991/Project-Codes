---
title: "Exam 2 Preparation"
author: "Kapil Garg"
date: "12/03/17"
output:
  html_document:
    toc: yes
---


# Sampling

```{r}
x.values <- c(2,4,6,8)
x.pmf <- c(0.2,0.3,0.3,0.2)

sample(x.values,size=10,prob = x.pmf,replace=TRUE) # Sample of Size 10

sample.values <- replicate(1000,sample(x.values,size=10,prob = x.pmf,replace=TRUE)) #1000 Samples of the same sample

```

# Confidence Interval for sample mean

1. Normal Population Distribution, sigma is know in this case

Manual Calculation:
Manual Formulae
Xbar+Za/2*sigma/sqrt(n)
Xbar-Za/2*sigma/sqrt(n)


SE=Sigma/sqrt(n)
UpperBound=qnorm(0.975,mean=2.2,sd=SE) here mean is the sample mean
LowerBound=qnorm(0.025,mean=2.2,sd=SE)

```{r}
SE = 0.35/sqrt(11)
UpperBound=qnorm(0.975,mean=2.2,sd=SE)
LowerBound=qnorm(0.025,mean=2.2,sd=SE)
```


2. Small population/ not normal/ sigma not known

Approach1: Parametric - Use a t-Student distribution

Approach2: Non Parametric - Create a sampling distribution of the means by using ???bootstrapping??? and then, find the interval using the quantiles of the distribution

## Approach1 T-Test:

For True Mean
df=n-1  --- n is the sample size
```{r}
salary=c(62, 71, 75, 47, 85, 85, 82, 78, 52, 53)
SE=sd(salary)/sqrt(10)
UB = mean(salary)+SE*qt(0.95,df=9)
LB = mean(salary)-SE*qt(0.95,df=9)

```


## Approach2 Non Parametric:
For ture mean
```{r}
means = replicate(1000,mean(sample(salary,10,replace=TRUE)))
UpperBound = quantile(means,prob=0.95)
LowerBound = quantile(means,prob=0.05)
hist(means,prob=T)
```
For true variance
```{r}
vars= replicate(1000,var(sample(salary,10,replace=TRUE)))
hist(vars)
UB=quantile(vars,prob=0.95)
LB= quantile(vars,prob=0.05)

```


# Confidence Interval for sample propotion

## When the porportion is large or normal i.e np>5 and np(1-p)>5

Manual Formulae
p+Za/2*sqrt(p(1-p)/n)
p-Za/2*sqrt(p(1-p)/n)

R Formulaes
```{r}
SEP=sqrt(0.25*0.75/100) # SE of proportion
UpperBound=qnorm(0.975,mean=0.25,sd=SEP) 
LowerBound=qnorm(0.025,mean=0.25,sd=SEP)
```

## When the porportion is small or not normal i.e it does not satisfy np>5 and np(1-p)>5

By Bootstrapping
```{r}
PP=replicate(1000,mean(sample(c(0,1),100,replace=TRUE,prob=c(0.75,0.25))))
hist(PP)
UB=quantile(PP,prob=0.975)
LB= quantile(PP,prob=0.025)
```

# Sampling Error
For the Mean---> Za/2*sqrt(p(1-p)/n)

## Size of sample by sample mean
n=Z^2*sigma^2/e^2
```{r}
Sample.Size=(qnorm(0.95)*sd(salary)/0.5)^2
```

## Size of sample by sample proportion
For the Proportion
n=Z^2*pi*(1-pi)/e^2
```{r}
Sample.Size=0.12*0.88*(qnorm(0.975)/0.03)^2
```



# Hypothesis Testing

## Test for normality
shapiro.test([data])


## If the data is normal

Acceptance region
Mu0-Za/2*sigma/sqrt(n)  Mu0+Za/2*sigma/sqrt(n)???_(???????2???)  ????/???????, ????_0+????_(???????2???)  ????/???????)

z <- qnorm(0.9) #For 90% confidence interval
Xbar<-z*sd/sqrt(n)+mo

z= (m-m0)/(sd/sqrt(n))

lower one-tailed test
pt(t,df=n-1) or pnorm(t) 

upper one-tailed test
1 - pt(t,df=n-1) or 1 - pnorm(t) 

two-tailed test
2*(1 - pt(t, df=n-1)) or 2*(1 - pnorm(t)), if t > 0 
2*pt(t, df=n-1) or 2*pnorm(t), if t < 0

t.test(x, y = NULL ,alternative = c("two.sided", "less", "greater"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...)


Ttest1 <- t.test(melanom$thick, alternative = "two.sided", mu = 255, confidence.level=0.95)
Ttest3 <- t.test(melanom$thick, alternative = "less", mu = 255, confidence.level=0.9)
Ttest4 <- t.test(melanom$thick, alternative = "greater", mu = 255, confidence.level=0.9)
Ttest1$conf.int



paramTest <- t.test(melanom$thick)
paramTest$conf.int



## If the data is not normal
Use the sample to create random draws from the distribution (Bootsrapping)
boot.one.per(x, parameter, null.hyp = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, type = NULL, R = 9999)

Manual Calculation

t<-qt(0.9,n-1) #For 90% confidence interval
Xbar<-t*sd/sqrt(n)+mo


Test for mean

library(wBoot)
bootTest <- boot.one.per(melanom$thick,mean)
bootTest$Confidence.limits
boot.one.per(melanom$thick, mean, null.hyp = 330, alternative = "less", conf.level = 0.98)

Test for median
boot.one.per(melanom$thick, median,null.hyp = 330, alternative = "less", conf.level = 0.98)

Test for standard deviation
boot.one.per(melanom$thick, sd,conf.level = 0.98)


# Two sample test

## T Test
main assumption is that each sample statistic is normally distributed

Check for normality
Reject null hypothesis to say that the distribution is normal

shapiro.test(energy$expend[energy$stature=="obese"])
shapiro.test(energy$expend[energy$stature=="lean"])

1. Unpaired dataset
t.test(energy$expend[energy$stature == "lean"],energy$expend[energy$stature == "obese"],paired = FALSE)

2. Paired Set
t.test(anorexia$Prewt[anorexia$Treat=="CBT"],anorexia$Postwt[anorexia$Treat=="CBT"],paired=TRUE)

NOT NORMAL


1.UNPAIRED TEST
require(wBoot)
boot.two.per(energy$expend[energy$stature == "lean"],energy$expend[energy$stature == "obese"],mean, stacked = FALSE)

2. Paired Test
boot.paired.per(x, y, variable = NULL, null.hyp = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, type = NULL, R = 9999)

# Correlation Test
H0: The correlation between the variables is 0
H1: The correlation between the variables is not equal to 0
cor.test(variable1,variable2)
boot.cor.per(x, y, null.hyp = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, type = NULL, R = 9999) # Safe Option which can always be applied


# Test for Variance
Always check for Normality first

var.test(Metal_Pipes$`2015`,Metal_Pipes$`2017`, conf.level = 0.99)
boot.ratio.sd.per(x, y, stacked = TRUE, variable = NULL, null.hyp = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, type = NULL, R = 9999)
boot.ratio.sd.per(na.omit(Metal_Pipes$`2015`),na.omit(Metal_Pipes$`2017`),stacked=FALSE)
test.result <- boot.ratio.sd.per(bergman, johnson.df$Lifetime, stacked = F, null.hyp = 1, alternative = 'less')


# Regression

## Linear Regression

reg1 = lm(Y~X)
summary(reg1)
fitted(reg1)

Residual error= Yactual-YPredictedCheck for Homoskedacity
bptest(reg1) 

SST=sum(Yactual-Ymean)^2   Total Variation
SSR=sum(Ypredicted-Ymean)^2   Explained Variation
SSE=sum(Yactual-Ypredicted)^2   Unexplained Variation

R Functions
sse = sum((fitted(reg1) - mean(Y))^2)
ssr = sum((fitted(reg1) - Y)^2)
sst = sse + ssr

RSquare=SSR/SST

Standard Error of the estimate: sqrt(SSE/(n-2))

H0: Beta1=0
H1: Beta1!=0

b1 is change in Y per unit change in x keeping all other factors constant

Predict a value
??? 
predict(model, data.frame(X=c(32.8)))
summary(model)$r.squared

predict(model, x.df,interval="predict")
predict(model, x.df,interval="confidence")

## Multiple Linear Regression

Basic command
 Multiple Linear Regression Example
fit <- lm(y ~ x1 + x2 + x3, data=mydata)
summary(fit) # show results

Other useful functions

coefficients(fit) # model coefficient
confint(fit, level=0.95) # CIs for model parameters
fitted(fit) # predicted values 
residuals(fit) # residuals

Confidence Interval
confint(reg1,'Price',level = 0.95)


Test of assumptions
gvlma(fit)

VIF
car::vif(fit)

Adjusted RSquare calculation
k- number of variable
n - number of rows
adj.r.squared.calc <-
1 - ((1 - r.squared.calc) * (n - 1) / (n - k - 1))

Manual Calculation of P value
Estimated_error<-model$coefficients[2]
Std_error<-coef(summary(model))[2,2]
t<- Estimated_error/Std_error
p<-2*(1 - pt(t, df=8))


## Logistic Regression

glm.default <- glm(default.indicator ~ customer.balance, family = "binomial")
summary(glm.default)
glm.coefficients <- coefficients(glm.default) #extracts ????_0 and ????_1
glm.coefficients[1] + glm.coefficients[2]*test.df #evaluates the input to the sigmoid function
round(exp(glm.coefficients[1] + glm.coefficients[2]*test.df)/(1+ exp(glm.coefficients[1] + glm.coefficients[2]*test.df) ),2) #probability estimates
round(predict(glm.default,test.df, type = "response"),2) # Predict 


Plots
plot(customer.balance, default.indicator)
curve(lm.coefficient[1] + lm.coefficient[2]*x, min(customer.balance, max(customer.balance)), add = TRUE, col = "red")
curve(exp(glm.coefficients[1] + glm.coefficients[2]*x) / ( 1 + exp(glm.coefficients[1] + glm.coefficients[2]*x) ), min(customer.balance , max(customer.balance)), add = TRUE, col = "blue")

????_1 gives the change in the log-odds per one-unit increase ????

Multiple logistic regression
glm.all <- glm(Default$default ~ Default$student + Default$balance + Default$income, family = "binomial")
summary(glm.all)

